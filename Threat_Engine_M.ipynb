{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Automated Threat Identidication and Assessment Module\n",
    "This is the Threat Identidication and Assessment Module that pulls data from selected resources on the internet and use different techniques to clean and get data to output threat level with ML tools like MonkeyLearn and Spacy.\n",
    "\n",
    "**NOTE**: Some sources have threats that have already been assessed so will directly give an output skipping the threat level classification stage"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Library imports\n",
    "import csv\n",
    "import re\n",
    "import json\n",
    "import bs4\n",
    "import spacy\n",
    "import requests\n",
    "import en_core_web_lg\n",
    "from urllib.request import urlopen as uReq\n",
    "from bs4 import BeautifulSoup as soup\n",
    "from datetime import datetime, date\n",
    "from IPython.display import clear_output\n",
    "from monkeylearn import MonkeyLearn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# MonkeyLearn API key and Model ID\n",
    "ml = MonkeyLearn('2b14f564c495a96ece74dc80df73514efb517d6c')\n",
    "model_id = 'cl_M7RxATri'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Custom Function to get the tag_name classification\n",
    "def getLvl(text):\n",
    "    data = [text]\n",
    "    result = ml.classifiers.classify(model_id, data)\n",
    "    if(len(result.body[0]['classifications']) > 0):\n",
    "        return result.body[0]['classifications'][0]['tag_name']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Loading spacy english model for entity identification\n",
    "nlp = en_core_web_lg.load() "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Custom funcation to merge locations identified using SpaCy's \"en\" model into one string\n",
    "def getLoc(text):\n",
    "    matches = [\"GPE\", \"LOC\", \"FAC\"]\n",
    "    doc = nlp(text)\n",
    "    G, F, L = \"\", \"\", \"\"\n",
    "\n",
    "    for e in doc.ents:\n",
    "        if \"GPE\" in e.label_:\n",
    "            if(G == \"\"):\n",
    "                G = e.text\n",
    "            if(G != \"\"):\n",
    "                if(e.text not in G):\n",
    "                    G = (G + \" \" + e.text)\n",
    "                \n",
    "        if \"LOC\" in e.label_:\n",
    "            if(L == \"\"):\n",
    "                L = e.text\n",
    "            if(L != \"\"):\n",
    "                if(e.text not in L):\n",
    "                    L = (L + \" \" + e.text)\n",
    "                \n",
    "        if \"FAC\" in e.label_:\n",
    "            if(F == \"\"):\n",
    "                F = e.text\n",
    "            if(F != \"\"):\n",
    "                if(e.text not in F):\n",
    "                    F = (e.text + \" \" + F)\n",
    "                    \n",
    "    return (F + \" \" + L + \" \" + G)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Location(s) in first string:  Sydney\n",
      "Location(s) in second string:  Sydney Australia Macquarie Park\n"
     ]
    }
   ],
   "source": [
    "# SpaCy Entity recognition\n",
    "location1 = getLoc(\"4 tested positive in Sydney\").lstrip()\n",
    "location2 = getLoc(u\"\"\"Macquarie University is a public research university based in Sydney\"\n",
    "                    \", Australia, in the suburb of Macquarie Park.\"\"\").lstrip()\n",
    "print(\"Location(s) in first string: \", location1)\n",
    "print(\"Location(s) in second string: \",location2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Demonstration of Monkey learn custom trained model\n",
    "lvla = getLvl(\"Scammers scam the population\")\n",
    "lvlb = getLvl(\"Coronavirus scammers steal $1.1 million from fearful victims\")\n",
    "print(lvla)\n",
    "print(lvlb)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Sources to scrape\n",
    "def getUrls():\n",
    "    url = []\n",
    "    #url.append('https://www.cyber.gov.au/acsc/view-all-content/alerts')\n",
    "    url.append('https://www.smh.com.au/topic/nsw-police-jdi')\n",
    "    url.append('https://www.smh.com.au/topic/sydney-crime-62n')\n",
    "    url.append('https://www.smh.com.au/topic/crime-5w4')\n",
    "    url.append('https://www.smh.com.au/topic/australian-federal-police-jnt')\n",
    "    url.append('https://www.9news.com.au/cyber-security')\n",
    "    url.append('https://www.9news.com.au/security')\n",
    "    url.append('https://www.rapid7.com/db/?q=&type=nexpose')\n",
    "    url.append('https://www.rapid7.com/db/?q=&type=nexpose&page=2')\n",
    "    url.append('https://www.rapid7.com/db/?q=&type=nexpose&page=3')\n",
    "    return url"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Reading the html data from the url to p_html\n",
    "def urlReq(my_url):\n",
    "    uClient = uReq(my_url)\n",
    "    p_html = uClient.read()\n",
    "    uClient.close()\n",
    "    return soup(p_html, \"html.parser\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Cyber.gov.au\n",
    "def cga(parsed_html):\n",
    "    #Splitting the html into a list of threats\n",
    "    articles = parsed_html.findAll(\"div\", {\"class\":\"views-row\"})\n",
    "    #deleting unrequired row from the list\n",
    "    del articles[0]\n",
    "    #Creating .csv file in the data folder and writing the required information from the list to .csv file\n",
    "    with open('data/cyber_gov_au.csv', 'w') as file:\n",
    "        w = csv.writer(file)\n",
    "        #write header row\n",
    "        w.writerow(['Threat_date', 'Title', 'Summary', 'Source', 'Location', 'Threat_level'])\n",
    "        for article in articles:\n",
    "            dateS = article.find(\"p\", {\"class\":\"acsc-date\"}).get_text()\n",
    "            title = article.find(\"p\", {\"class\":\"acsc-title\"}).get_text()\n",
    "            summary = article.find(\"p\", {\"class\":\"acsc-summary\"}).get_text()\n",
    "            src_link = (\"https://www.cyber.gov.au\" + article.a['href'])\n",
    "\n",
    "            d = datetime.strptime(dateS, '%d %b %Y ')\n",
    "\n",
    "            uClient = uReq(src_link)\n",
    "            detailed_page = uClient.read()\n",
    "            uClient.close()\n",
    "            dp_parsed = soup(detailed_page, \"html.parser\")\n",
    "            Threat = dp_parsed.find(\"div\", {\"\"\"class\":\"field field--name-field-alert-status \n",
    "                                            field--type-entity-reference field--label-inline\"\"\"})\n",
    "            lvl = Threat.find(\"div\", {\"class\":\"field__item\"}).get_text()\n",
    "\n",
    "            summary = summary.replace(\",\", \";\")\n",
    "            w.writerow([d.date(), title, summary, src_link, \"\", lvl])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Sydney Morning Herald\n",
    "def smh(parsed_html, url):\n",
    "    articles = parsed_html.findAll(\"div\", {\"class\":\"_2g9tm\"})\n",
    "    #Creating csv file\n",
    "    fname = \"smh_\" + ('_'.join(re.findall(r\"(\\w+)\", url.rsplit('/', 1)[-1])))\n",
    "    with open ('data/%s.csv' % (fname), 'w') as file:\n",
    "        w = csv.writer(file)\n",
    "        #write header row\n",
    "        w.writerow(['Threat_date', 'Title', 'Summary', 'Source', 'Location', 'Threat_level'])\n",
    "        #grabbing news article information\n",
    "        for article in articles:\n",
    "            title = article.find(\"a\",{\"data-test\": \"article-link\"}).get_text()\n",
    "            summary = article.find(\"p\",{\"class\": \"_3b7W- _3XEsE\"}).get_text()\n",
    "            src_link = (\"https://www.smh.com.au\" + article.find(\"a\",{\"data-test\": \"article-link\"}).get('href'))\n",
    "            dateS = article.find(\"time\", {\"class\": \"_2_zR-\"}).get_text()\n",
    "            title = title.replace('\"', \"\")\n",
    "            title = title.replace(\"'\", \"\")\n",
    "            summary = summary.replace('\"', \"\")\n",
    "            summary = summary.replace(\"'\", \"\")\n",
    "            \n",
    "            loc = getLoc(title + \". \" + summary).lstrip()\n",
    "            lvl = getLvl(title + \". \" + summary)\n",
    "            \n",
    "            d = date.today()\n",
    "            if(\"Today\" in dateS or \"ago\" in dateS):\n",
    "                w.writerow([d, title, 0,summary, url, \"-\"])\n",
    "            else:\n",
    "                d = datetime.strptime(dateS, '%B %d, %Y')\n",
    "                w.writerow([d.date(), title, summary, src_link, loc, lvl])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 9News\n",
    "def nine(parsed_html, url):\n",
    "    feed = parsed_html.find(\"div\", {\"data-feed\":\"default\"})\n",
    "    articles = feed.findAll(\"div\", {\"class\":\"story__details\"})\n",
    "    #Creating csv file\n",
    "    fname = \"9news_\" + ('_'.join(re.findall(r\"(\\w+)\", url.rsplit('/', 1)[-1])))\n",
    "    with open ('data/%s.csv' % (fname), 'w') as file:\n",
    "        w = csv.writer(file)\n",
    "        #write header row \n",
    "        w.writerow(['Threat_date', 'Title', 'Summary', 'Source', 'Location', 'Threat_level'])\n",
    "        #grabbing news article information\n",
    "        for article in articles:\n",
    "            if(article.find(\"div\",{\"class\": \"widget widget-ad feed__ad\"})):\n",
    "                pass\n",
    "            else:\n",
    "                title = article.find(\"span\",{\"class\": \"story__headline__text\"}).get_text()\n",
    "                summary = article.find(\"div\", {\"class\": \"story__abstract\"}).get_text()\n",
    "                src_link = article.a['href']\n",
    "                dateS = article.find(\"time\", {\"class\": \"story__time\"}).get_text()\n",
    "                title = title.replace('\"', \"\")\n",
    "                title = title.replace(\"'\", \"\")\n",
    "                summary = summary.replace('\"', \"\")\n",
    "                summary = summary.replace(\"'\", \"\")\n",
    "                \n",
    "                loc = getLoc(title + \". \" + summary).lstrip()\n",
    "                lvl = getLvl(title + \". \" + summary)\n",
    "                \n",
    "                d = date.today()\n",
    "                if(\"Today\" in dateS or \"ago\" in dateS):\n",
    "                    w.writerow([d, title, 0,summary, url, \"-\"])\n",
    "                else:\n",
    "                    d = datetime.strptime(dateS, '%I:%M%p %b %d, %Y')\n",
    "                    w.writerow([d.date(), title, summary, src_link, loc, lvl])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Rapid7\n",
    "def rapid(parsed_html, url):\n",
    "    articles = parsed_html.findAll(\"a\", {\"class\":\"vulndb__result resultblock\"})\n",
    "    #Creating csv file\n",
    "    fname = \"rapid7_\" + ('_'.join(re.findall(r\"(\\w+)\", url.rsplit(\"nexpose\", 1)[-1])))\n",
    "    with open ('data/%s.csv' % (fname), 'w') as file:\n",
    "        w = csv.writer(file)\n",
    "        #Creating Headers for csv file \n",
    "        w.writerow(['Threat_date', 'Title', 'Summary', 'Source', 'Location', 'Threat_level'])\n",
    "        #grabbing news threat information\n",
    "        for article in articles:\n",
    "            title = article.find(\"div\",{\"class\": \"resultblock__info-title\"}).get_text()\n",
    "            src_link = (\"https://www.rapid7.com\" + article.get('href'))\n",
    "            meta = article.find(\"div\", {\"class\": \"resultblock__info-meta\"}).get_text()\n",
    "            meta = meta.lstrip().rstrip()\n",
    "            dateS = meta[11:30].rstrip()\n",
    "            lvl = int(meta[meta.find(\"Severity:\")+9:].split()[0].lstrip())\n",
    "            d = date.today()\n",
    "            if(\"Today\" in dateS or \"ago\" in dateS):\n",
    "                w.writerow([d, title, 0,\"-\", url, \"-\"])\n",
    "            else:\n",
    "                d = datetime.strptime(dateS, '%B %d, %Y')\n",
    "                w.writerow([d.date(), title.lstrip().rstrip(), \"\", src_link, \"\", lvl])\n",
    "                "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Main program"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get list of URLs\n",
    "urls = getUrls()\n",
    "c = 0 # Counter\n",
    "\n",
    "# Loop through urls and call funcations to scrape data and identify entities using ML\n",
    "for url in urls:\n",
    "    pHtml = urlReq(url)\n",
    "    if('cyber.gov.au' in url):\n",
    "        cga(pHtml)\n",
    "    if('smh' in url):\n",
    "        smh(pHtml, url)\n",
    "    if('9news' in url):\n",
    "        nine(pHtml, url)\n",
    "    if('rapid7' in url):\n",
    "        rapid(pHtml, url)\n",
    "    c+=1\n",
    "    clear_output(wait=True)\n",
    "    print((str(c/len(urls)*100) +  \"%\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
